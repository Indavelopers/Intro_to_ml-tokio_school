{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regresión lineal multivariable: Función de coste y gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ¿Qué vamos a hacer?\n",
    "\n",
    "- Implementar la función de coste para regresión lineal multivariable\n",
    "- Implementar la optimización de dicha función de coste por gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tarea 1: Implementar la función de coste para regresión lineal multivariable\n",
    "\n",
    "En esta tarea, debes implementar la función de coste para regresión lineal multivariable en Python usando Numpy. La función de coste debe seguir la función incluida en las diapositvas y en el manual del curso.\n",
    "\n",
    "Para ello, rellena el código de la siguiente celda:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implementa la función de coste siguiendo la siguiente plantilla\n",
    "\n",
    "def cost_function(x, y, theta):\n",
    "    \"\"\" Computa la función de coste para el dataset y coeficientes considerados.\n",
    "    \n",
    "    Argumentos posicionales:\n",
    "    x -- array 2D de Numpy con los valores de las variables independientes de los ejemplos, de tamaño m x n\n",
    "    y -- array 1D de Numpy con la variable dependiente/objetivo, de tamaño m x 1\n",
    "    theta -- array 1D de Numpy con los pesos de los coeficientes del modelo, de tamaño 1 x n (vector fila)\n",
    "    \n",
    "    Devuelve:\n",
    "    j -- float con el coste para dicho array theta\n",
    "    \"\"\"\n",
    "    m = [...]\n",
    "    \n",
    "    # Recuerda comprobar las dimensiones de la multiplicación matricial para hacerla correctamente\n",
    "    j = [...]\n",
    "    \n",
    "    return j"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para comprobar tu implementación, rescata tu código del notebook anterior acerca de datasets sintéticos y sigue las siguientes instrucciones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Genera un dataset sintético, sin término de error, de la forma que escojas\n",
    "\n",
    "m = 0\n",
    "n = 0\n",
    "\n",
    "X = [...]\n",
    "\n",
    "Theta_verd = [...]\n",
    "\n",
    "Y = [...]\n",
    "\n",
    "# Comprueba los valores y dimensiones (forma o \"shape\") de los vectores\n",
    "print('Theta a estimar:')\n",
    "print()\n",
    "\n",
    "print('Primeras 10 filas y 5 columnas de X e Y:')\n",
    "print()\n",
    "print()\n",
    "\n",
    "print('Dimensiones de X e Y:')\n",
    "print('shape', 'shape')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como el dataset sintético no tiene término de error, la función de coste para la Theta correcta debe ser exactamente 0, aumentando su valor según nos alejamos de la misma.\n",
    "\n",
    "Comprueba tu implementación de la función de coste comprobando su valor con diferentes valores de su argumento theta, comprobando varios valores desde la Theta incorrecta a valores más alejados de la misma:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Comprueba la implementación de tu función de coste\n",
    "\n",
    "theta = Theta_verd    # Modifica y comprueba varios valores de theta\n",
    "\n",
    "j = cost_function(X, Y, theta)\n",
    "\n",
    "print('Coste del modelo:')\n",
    "print(j)\n",
    "print('Theta comprobado y Theta real:')\n",
    "print(theta)\n",
    "print(Theta_verd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tarea 2: Implementar la optimización de dicha función de coste por gradient descent\n",
    "\n",
    "Ahora vamos a resolver la optimización de dicha función de coste para entrenar el modelo, mediante el método de gradient descent. El modelo se considerará entrenado cuando su función de coste haya alcanzado un valor mínimo.\n",
    "\n",
    "Para ello, de nuevo, rellena la plantilla de código de la siguiente celda:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implementa la función que entrena el modelo por gradient descent\n",
    "\n",
    "def gradient_descent(x, y, theta, alpha, e, iter_):\n",
    "    \"\"\" Entrena el modelo optimizando su función de coste por gradient descent\n",
    "    \n",
    "    Argumentos posicionales:\n",
    "    x -- array 2D de Numpy con los valores de las variables independientes de los ejemplos, de tamaño m x n\n",
    "    y -- array 1D de Numpy con la variable dependiente/objetivo, de tamaño m x 1\n",
    "    theta -- array 1D de Numpy con los pesos de los coeficientes del modelo, de tamaño 1 x n (vector fila)\n",
    "    alpha -- float, ratio de entrenamiento\n",
    "    \n",
    "    Argumentos nombrados (keyword):\n",
    "    e -- float, diferencia mínima entre iteraciones para declarar que el entrenamiento ha convergido finalmente\n",
    "    iter_ -- int/float, nº de iteraciones\n",
    "    \n",
    "    Devuelve:\n",
    "    j_hist -- list/array con la evolución de la función de coste durante el entrenamiento\n",
    "    theta -- array de Numpy con el valor de theta en la última iteración\n",
    "    \"\"\"\n",
    "    # TODO: declara unos valores por defecto para e e iter_ en los argumentos nombrados (keyword) de la función\n",
    "    \n",
    "    iter_ = int(iter_)    # Si has declarado iter_ en notación científica (1e3) o float (1000.), conviértelo\n",
    "    \n",
    "    # Inicializa j_hist como una list o un array de Numpy. Recuerda que no sabemos qué tamaño tendrá finalmente\n",
    "    j_hist = [...]\n",
    "    \n",
    "    m, n = [...]    # Obtén m y n a partir de las dimensiones de X\n",
    "    \n",
    "    for k in [...]:    # Itera sobre el nº de iteraciones máximo\n",
    "        theta_iter = [...]    # Declara una theta para cada iteración, ya que debemos actualizarla\n",
    "        \n",
    "        for j in [...]:    # Itera sobre el nº de características\n",
    "            # Actualiza theta_iter para cada característica, según la derivada de la función de coste\n",
    "            # Incluye el ratio de entrenamiento alpha\n",
    "            # Cuidado con las multiplicaciones matriciales, su órden y dimensiones\n",
    "            theta_iter[j] = theta[j] - [...]\n",
    "            \n",
    "        theta = theta_iter\n",
    "        \n",
    "        cost = cost_function([...])    # Calcula el coste para la iteración de theta actual\n",
    "        \n",
    "        j_hist[...]    # Añade el coste de la iteración actual al histórico de costes\n",
    "        \n",
    "        # Comprueba si la diferencia entre el coste de la iteración actual y el de la última iteración en valor\n",
    "        # absoluto son menores que la diferencia mínima para declarar convergencia, e\n",
    "        if k > 0 and [...]:\n",
    "            print('Converge en la iteración nº: ', k)\n",
    "            \n",
    "            break\n",
    "    else:\n",
    "        print('Nº máx. de iteraciones alcanzado')\n",
    "        \n",
    "    return j_hist, theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para comprobar tu implementación, de nuevo, usa varios valores de Theta, tanto la Theta correcta como valores cada vez más alejados de ella, y comprueba que finalmente el modelo converge a la Theta correcta:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Comprueba tu implementación entrenando un modelo sobre el dataset sintético creado previamente\n",
    "\n",
    "# Crea una theta inicial con un valor dado.\n",
    "# Primero usa el valor de Theta correcto, luego unos valores cada vez más alejados.\n",
    "# Finalmente, comprueba también tu implementación con valores de theta_ini aleatorios\n",
    "theta_ini = [...]\n",
    "\n",
    "print('Theta inicial:')\n",
    "print(theta_ini)\n",
    "\n",
    "alpha = 1e-1\n",
    "e = 1e-3\n",
    "iter_ = 1e3    # Comprueba que tu función puede admitir valores float o modifícalo\n",
    "\n",
    "print('Hiper-arámetros usados:')\n",
    "print('Alpha:', alpha, 'Error máx.:', e, 'Nº iter', iter_)\n",
    "\n",
    "t = time.time()\n",
    "j_hist, theta_final = gradient_descent([...])\n",
    "\n",
    "print('Tiempo de entrenamiento (s):', time.time() - t)\n",
    "\n",
    "# TODO: completar\n",
    "print('\\nÚltimos 10 valores de la función de coste')\n",
    "print(j_hist[...])\n",
    "print('\\Coste final:')\n",
    "print(j_hist[...])\n",
    "print('\\nTheta final:')\n",
    "print(theta_final)\n",
    "\n",
    "print('Valores verdaderos de Theta y diferencia con valores entrenados:')\n",
    "print(Theta_verd)\n",
    "print(theta_final - Theta_verd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Representar la función de coste\n",
    "\n",
    "Representa gráficamente el histórico de la función de coste para comprobar tu implementación:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TOOD: Representa gráficamente la función de coste vs el nº de iteraciones\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.title('Función de coste')\n",
    "plt.xlabel('nº iteraciones')\n",
    "plt.ylabel('coste')\n",
    "\n",
    "plt.plot([...])    # Completa\n",
    "\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para comprobar completamente la implementación de dichas funciones, modifica el dataset sintético original para comprobar que la función de coste y el entrenamiento por gradient descent siguen funcionando correctamente.\n",
    "\n",
    "P. ej., modifica el nº de ejemplos y el nº de características.\n",
    "\n",
    "También añádele de nuevo un término de error a la Y. En este caso, puede que la Theta inicial y la final no concuerden del todo, ya que hemos introducido error o \"ruido\" en el dataset de entrenamiento.\n",
    "\n",
    "Por último, comprueba todos los hiper-parámetros de tu implementación. Utiliza varios valores de alpha, e, nº de iteraciones, etc., y comprueba que los resultados son los esperados."
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "common-cpu.m54",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m54"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
