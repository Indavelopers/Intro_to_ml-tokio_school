{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regresión lineal: Regularización\n",
    "\n",
    "## ¿Qué vamos a hacer?\n",
    "- Implementar la función de coste regularizada para la regresión lineal multivariable\n",
    "- Implementar la regularización para el gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creación de un dataset sintético\n",
    "\n",
    "Para comprobar tu implementación de una función de coste y gradient descent regularizado, rescata tus celdas de los notebooks anteriores acreca de datasets sintéticos y genera un dataset para este ejercicio.\n",
    "\n",
    "No olvides añadirle un término de bias a la *X* y un término de error a la *Y*, inicializado a 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Genera un dataset sintéitico manualmente, con término de bias y término de error inicializado a 0\n",
    "\n",
    "m = 1000\n",
    "n = 3\n",
    "\n",
    "X = [...]\n",
    "\n",
    "Theta_verd = [...]\n",
    "\n",
    "Y = [...]\n",
    "\n",
    "# Comprueba los valores y dimensiones de los vectores\n",
    "print('Theta a estimar y sus dimensiones:')\n",
    "print()\n",
    "print()\n",
    "\n",
    "print('Primeras 10 filas y 5 columnas de X e Y:')\n",
    "print()\n",
    "print()\n",
    "\n",
    "print('Dimensiones de X e Y:')\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Función de coste regularizada\n",
    "\n",
    "Ahora vamos a modificar nuestra implementación de la función de coste para añadirle el término de regularización.\n",
    "\n",
    "Recuerda que la función de coste regularizada es:\n",
    "\n",
    "$J_\\theta = \\frac{1}{2m} [\\sum\\limits_{i=0}^{m} (h_\\theta(x^i)-y^i)^2 + \\lambda \\sum\\limits_{j=1}^{n} \\theta^2_j]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implementa la función de coste regularizada siguiendo la siguiente plantilla\n",
    "\n",
    "def cost_function(x, y, theta, lambda_=0.):\n",
    "    \"\"\" Computa la función de coste para el dataset y coeficientes considerados.\n",
    "    \n",
    "    Argumentos posicionales:\n",
    "    x -- array 2D de Numpy con los valores de las variables independientes de los ejemplos, de tamaño m x n\n",
    "    y -- array 1D de Numpy con la variable dependiente/objetivo, de tamaño m x 1\n",
    "    theta -- array 1D de Numpy con los pesos de los coeficientes del modelo, de tamaño 1 x n (vector fila)\n",
    "    \n",
    "    Argumentos nombrados:\n",
    "    lambda -- float con el parámetro de regularización\n",
    "    \n",
    "    Devuelve:\n",
    "    j -- float con el coste regularizada para dicho array theta\n",
    "    \"\"\"\n",
    "    m = [...]\n",
    "    \n",
    "    # Recuerda comprobar las dimensiones de la multiplicación matricial para hacerla correctamente\n",
    "    # Recuerda no regularizar el coeficiente del parámetro bias (primer valor de theta)\n",
    "    j = [...]\n",
    "    \n",
    "    return j"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como el dataset sintético tiene el término de error a 0, la función de coste para la *Theta_verd* con parámetro *lambda* 0 debe ser exactamente 0.\n",
    "\n",
    "Al igual que antes, según nos alejamos con valores de *theta* diferentes, el coste debe aumentar. Del mismo modo, a mayor parámetro de regularización, mayor penalización y coste, y a mayor valor de *Theta*, también mayor penalización y coste.\n",
    "\n",
    "Comprueba tu implementación en estas 5 circunstancias:\n",
    "- Usando *Theta_verd* y con *lambda* a 0, el coste debe seguir siendo 0.\n",
    "- Con *lambda* 0 aún, según los valores de *theta* se alejen de *Theta_verd*, el coste debe ser mayor.\n",
    "- Usando *Theta_verd* y con *lambda* distinta de 0, el coste ahora debe ser mayor de 0.\n",
    "- Con *lambda* distinta de 0, para una *theta* distinta a *Theta_verd* el coste debe ser mayor que con *lambda* igual a 0.\n",
    "- Con *lambda* distinta de 0, cuanto mayores sean los valores de los coeficientes de *theta* (en sentido positivo o negativo*), mayor será la penalización y el coste.\n",
    "\n",
    "Recordamos que el valor de *lambda* siempre debe ser positivo y menor de 0: [0, 1e-1, 3e-1, 1e-2, 3e-2, ...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Comprueba la implementación de tu función de coste regularizada en dichas circunstancias\n",
    "\n",
    "theta = Theta_verd    # Modifica y comprueba varios valores de theta\n",
    "\n",
    "j = cost_function(X, Y, theta)\n",
    "\n",
    "print('Coste del modelo:')\n",
    "print(j)\n",
    "print('Theta comprobado y Theta real:')\n",
    "print(theta)\n",
    "print(Theta_verd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anota tus resultados en esta celda:\n",
    "1. Resultado1\n",
    "1. Resultado2\n",
    "1. Resultado3\n",
    "1. Resultado4\n",
    "1. Resultado5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient descent regularizado\n",
    "\n",
    "Ahora vamos a regularizar también el entrenamiento por gradient descent. Vamos a modificar las actualizaciones de *Theta* para que ahora contengan también el parámetro de regularización *lambda*:\n",
    "\n",
    "$\\theta_0 := \\theta_0 - \\alpha \\frac{1}{m} \\sum_{i=0}^{m}(h_\\theta (x^i) - y^i) x_0^i \\\\\n",
    "\\theta_j := \\theta_j - \\alpha [\\frac{1}{m} \\sum_{i=0}^{m}(h_\\theta (x^i) - y^i) x_j^i + \\frac{\\lambda}{m} \\theta_j] \\\\\n",
    "\\theta_j := \\theta_j (1 - \\alpha \\frac{\\lambda}{m}) - \\alpha \\frac{1}{m} \\sum_{i=0}^{m}(h_\\theta (x^i) - y^i) x_j^i \\\\\n",
    "j \\in [1, n]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implementar la función que entrena el modelo por gradient descent regularizado\n",
    "\n",
    "def gradient_descent(x, y, theta, alpha, lambda_=0., e, iter_):\n",
    "    \"\"\" Entrena el modelo optimizando su función de coste por gradient descent\n",
    "    \n",
    "    Argumentos posicionales:\n",
    "    x -- array 2D de Numpy con los valores de las variables independientes de los ejemplos, de tamaño m x n\n",
    "    y -- array 1D de Numpy con la variable dependiente/objetivo, de tamaño m x 1\n",
    "    theta -- array 1D de Numpy con los pesos de los coeficientes del modelo, de tamaño 1 x n (vector fila)\n",
    "    alpha -- float, ratio de entrenamiento\n",
    "    \n",
    "    Argumentos nombrados (keyword):\n",
    "    lambda -- float con el parámetro de regularización\n",
    "    e -- float, diferencia mínima entre iteraciones para declarar que el entrenamiento ha convergido finalmente\n",
    "    iter_ -- int/float, nº de iteraciones\n",
    "    \n",
    "    Devuelve:\n",
    "    j_hist -- list/array con la evolución de la función de coste durante el entrenamiento\n",
    "    theta -- array de Numpy con el valor de theta en la última iteración\n",
    "    \"\"\"\n",
    "    # TODO: declara unos valores por defecto para e e iter_ en los argumentos nombrados (keyword) de la función\n",
    "    \n",
    "    iter_ = int(iter_)    # Si has declarado iter_ en notación científica (1e3) o float (1000.), conviértelo\n",
    "    \n",
    "    # Inicializa j_hist como una list o un array de Numpy. Recuerda que no sabemos qué tamaño tendrá finalmente\n",
    "    j_hist = [...]\n",
    "    \n",
    "    m, n = [...]    # Obtén m y n a partir de las dimensiones de X\n",
    "    \n",
    "    for k in [...]:    # Itera sobre el nº de iteraciones máximo\n",
    "        theta_iter = [...]    # Declara una theta para cada iteración, ya que debemos actualizarla\n",
    "        \n",
    "        for j in [...]:    # Itera sobre el nº de características\n",
    "            # Actualiza theta_iter para cada característica, según la derivada de la función de coste\n",
    "            # Incluye el ratio de entrenamiento alpha\n",
    "            # Cuidado con las multiplicaciones matriciales, su órden y dimensiones\n",
    "            \n",
    "            if j > 0:\n",
    "                pass    # Regulariza todo coeficiente excepto el del parámetro bias (primer coef.)\n",
    "            \n",
    "            theta_iter[j] = theta[j] - [...]\n",
    "            \n",
    "        theta = theta_iter\n",
    "        \n",
    "        cost = cost_function([...])    # Calcula el coste para la iteración de theta actual\n",
    "        \n",
    "        j_hist[...]    # Añade el coste de la iteración actual al histórico de costes\n",
    "        \n",
    "        # Comprueba si la diferencia entre el coste de la iteración actual y el de la última iteración en valor\n",
    "        # absoluto son menores que la diferencia mínima para declarar convergencia, e\n",
    "        if k > 0 and [...]:\n",
    "            print('Converge en la iteración nº: ', k)\n",
    "            \n",
    "            break\n",
    "    else:\n",
    "        print('Nº máx. de iteraciones alcanzado')\n",
    "        \n",
    "    return j_hist, theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Nota*: Recuerda que las plantillas de código son sólo una ayuda. En ocasiones, puede que quieras usar un código diferente con la misma funcionalidad, p. ej. que itere sobre los elementos de otra forma, etc. ¡Síentete libre de modificarlos a tu antojo!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para comprobar tu implementación de nuevo, comprueba con *lambda* a 0 usando varios valores de *Theta*, tanto la *Theta_verd* como valores cada vez más alejados de ella, y comprueba que finalmente el modelo converge a la *Theta_verd*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Comprueba tu implementación entrenando un modelo sobre el dataset sintético creado previamente\n",
    "\n",
    "# Crea una theta inicial con un valor dado.\n",
    "theta_ini = [...]\n",
    "\n",
    "print('Theta inicial:')\n",
    "print(theta_ini)\n",
    "\n",
    "alpha = 1e-1\n",
    "lambda_ = 0.\n",
    "e = 1e-3\n",
    "iter_ = 1e3    # Comprueba que tu función puede admitir valores float o modifícalo\n",
    "\n",
    "print('Hiper-arámetros usados:')\n",
    "print('Alpha:', alpha, 'Error máx.:', e, 'Nº iter', iter_)\n",
    "\n",
    "t = time.time()\n",
    "j_hist, theta_final = gradient_descent([...])\n",
    "\n",
    "print('Tiempo de entrenamiento (s):', time.time() - t)\n",
    "\n",
    "# TODO: completar\n",
    "print('\\nÚltimos 10 valores de la función de coste')\n",
    "print(j_hist[...])\n",
    "print('\\Coste final:')\n",
    "print(j_hist[...])\n",
    "print('\\nTheta final:')\n",
    "print(theta_final)\n",
    "\n",
    "print('Valores verdaderos de Theta y diferencia con valores entrenados:')\n",
    "print(Theta_verd)\n",
    "print(theta_final - Theta_verd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora comprueba de nuevo el entrenamiento de un modelo en algunas de las circunstancias anteriores:\n",
    "- Usando una *theta_ini* aleatoria y con *lambda* a 0, el coste final debe seguir siendo cercano a 0 y la *theta* final cercana a *Theta_verd*.\n",
    "- Usando una *theta_ini* aleatoria y con *lambda* pequeña y distinta de 0, el coste final debe ser cercano a 0, aunque el modelo puede empezar a perder precisión.\n",
    "- Según aumenta el valor de *lambda*, el modelo perderá más precisión.\n",
    "\n",
    "Para ello recuerda que puedes modificar los valores de las celdas y reejecutarlas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anota tus resultados en esta celda:\n",
    "1. Resultado1\n",
    "1. Resultado2\n",
    "1. Resultado3"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "common-cpu.m54",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m54"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
