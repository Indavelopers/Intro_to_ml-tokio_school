{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA: Dataset caras de Olivetti\n",
    "\n",
    "## ¿Qué vamos a hacer?\n",
    "- Reducir la dimensionalidad de un dataset de alta dimensionalidad como el LFW\n",
    "- Escoger un el subconjunto mínimo de vectores\n",
    "- Representar los vectores principales\n",
    "\n",
    "En el ejercicio anterior vimos cómo implementar PCA con Scikit-learn para reducir la dimensionalidad, particularmente de cara a la representación de los datos.\n",
    "\n",
    "En este ejercicio nos vamos a centrar en encontrar el número mínimo de componentes principales al que podemos reducir la dimensionalidad del dataset perdiendo el mínimo de información o varianza posible.\n",
    "\n",
    "Para ello, vamos a utilizar el dataset de (caras de Olivetti)[https://scikit-learn.org/stable/datasets/real_world.html#olivetti-faces-dataset] para poder explorar la representación gráfica de los componentes principales en clasificación de imágenes y su relación con las imágenes originales.\n",
    "\n",
    "*NOTA*: Para este ejercicio te puedes basar las siguientes páginas de la documentación de Scikit-learn:\n",
    "- [sklearn.decomposition.PCA](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html).\n",
    "- [Faces dataset decompositions](https://scikit-learn.org/stable/auto_examples/decomposition/plot_faces_decomposition.html).\n",
    "\n",
    "*PD: Habitualmente, la labor de un ingeniero de software es saber buscar en la documentación y ejemplos en internet cómo hacer algo, y modificar dicho código para su caso de uso. Por eso, en este y otros ejercicios te invitamos a trabajar componiendo un código a través de partes de otros ejemplos =).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Importa todas las librerías necesarias aquí\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "rng = np.random.RandomState(42)\n",
    "\n",
    "fignum = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## El dataset de caras de Olivetti\n",
    "\n",
    "Primero vamos a descargar el dataset de caras de Olivetti, extraer las 6 primeras caras y representarlas en una gráfica.\n",
    "\n",
    "Para ello, sigue las instrucciones para completar la siguiente celda de código:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuración de la repesentación de las imágenes\n",
    "\n",
    "n_col = 3    # Nº de columnas y filas de la gráfica\n",
    "n_row = 2    # Gráfica de 2x3 imágenes\n",
    "image_shape = (64, 64)    # Tamaño en px de la imagen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Descarga el dataset de caras de Olivetti, extrae las 6 primeras caras y represéntalas en una gráfica\n",
    "\n",
    "# Descarga las imágenes del dataset\n",
    "# Para este ejemplo no vamos a clasificar el dataset, por lo que podemos descartar las clases\n",
    "faces, _ = datasets.fetch_olivetti_faces(return_X_y=True, shuffle=True, random_state=rng)\n",
    "\n",
    "# Extrae el nº de ejemplos y características del mismo, m y n\n",
    "m, n = [...]\n",
    "\n",
    "# Extrae las 6 primeras imágenes\n",
    "faces = [...]\n",
    "\n",
    "# Centra las imágenes\n",
    "# Para ello, puedes basarte en el ejemplo mencionado previamente\n",
    "faces_centered = [...]\n",
    "\n",
    "# Representa gráficamente las 6 primeras imágenes\n",
    "plt.figure(fignum, figsize=(2. * n_col, 2.25 * n_row))\n",
    "\n",
    "plt.suptitle('6 primeras imágenes originales')\n",
    "\n",
    "for i, face in enumerate(faces_centered):\n",
    "    plt.subplot(n_row, n_col, i + 1)\n",
    "    vmax = max(face.max(), -face.min())\n",
    "    \n",
    "    plt.imshow(face.reshape(image_reshape), cmap=plt.cm.gray, interpolation='nearest', vmin=-vmax, vmax=vmax)\n",
    "\n",
    "    plt.xticks(())\n",
    "    plt.yticks(())\n",
    "\n",
    "plt.subplots_adjust(0.01, 0.05, 0.99, 0.93, 0.04, 0.)\n",
    "\n",
    "fignum += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reducción de dimensionalidad\n",
    "\n",
    "Ahora vamos a comprobar cómo es la representación gráfica de una reducción de dimensionalidad en imágenes.\n",
    "\n",
    "Al pasar de las imágenes originales a los componentes principales, veremos cómo el PCA muestra los rasgos principales de cada imagen.\n",
    "\n",
    "Sigue las instrucciones de la siguiente celda para representar los 6 primeros componentes principales por PCA de dichas caras:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Representa los 6 primeros componentes principales de las caras y represéntalas gráficamente\n",
    "\n",
    "pca_6 = PCA(n_components=6, svd_solver='randomized', whiten=True)\n",
    "\n",
    "components = pca_6.fit([...]).components_\n",
    "\n",
    "# Representa los componentes principales para las 6 primeras caras\n",
    "plt.figure(fignum, figsize=(2. * n_col, 2.25 * n_row))\n",
    "\n",
    "[...]\n",
    "\n",
    "fignum += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Escoger el nº óptimo de componentes principales\n",
    "\n",
    "En el ejemplo anterior hemos obtenido los 6 primeros componentes principales. Sin embargo, dicho número de componentes ha sido un número escogido a discreción.\n",
    "\n",
    "Habitualmente, para reducir la dimensionalidad de un dataset a la hora de entrenar un modelo de ML, queremos reducirla al máximo, encontrando el número mínimo de componentes que mantiene el máximo de información o varianza del dataset original por el método [MLE de Minka](https://vismod.media.mit.edu/tech-reports/TR-514.pdf).\n",
    "\n",
    "Para ello, antes de continuar, hazte las siguientes preguntas:\n",
    "- *¿Qué representa el parámetro de \"sdv_solver\" del método PCA()?*\n",
    "- *¿Y el de \"n_components\", y su relación con \"sdv_solver\"?*\n",
    "- *¿Qué método implementa el \"n_components == 'mle'\"?*\n",
    "- *¿Qué representan los atributos del método PCA()? \"components_\", \"explained_variance_\", etc.*\n",
    "- *¿En qué orden se ordenan los \"components_\" que devuelve el PCA()?*\n",
    "\n",
    "Para encontrar el número mínimo de componentes principales, sigue las instrucciones para completar la siguiente celda:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Escoge el nº óptimo de componentes principales\n",
    "\n",
    "pca_min = PCA(n_components='mle')\n",
    "\n",
    "components = pca_min.fit([...]).components_\n",
    "\n",
    "# Representa los componentes principales para las 6 primeras caras\n",
    "plt.figure(fignum, figsize=(2. * n_col, 2.25 * n_row))\n",
    "\n",
    "[...]\n",
    "\n",
    "fignum += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El método MLE de Minka intenta establecer un heurístico para escoger el nº de componentes.\n",
    "\n",
    "También podemos escoger nosotros un porcentaje mínimo de varianza o información que queremos preservar del dataset, y el método PCA devolverá el nº mínimo de componentes que preservan dicho porcentaje o más.\n",
    "\n",
    "Para ello, sigue las instrucciones de la siguiente celda:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Escoge el nº óptimo de componentes principales\n",
    "\n",
    "min_var = 0.9    # Varianza mín. a preservar\n",
    "\n",
    "pca_min_var = PCA(n_components=min_var, svd_solver='full')\n",
    "\n",
    "pca_min_var_fitted = pca_min_var.fit([...])\n",
    "\n",
    "# Representa los componentes principales para las 6 primeras caras\n",
    "plt.figure(fignum, figsize=(2. * n_col, 2.25 * n_row))\n",
    "\n",
    "[...]\n",
    "\n",
    "fignum += 1\n",
    "\n",
    "# Analiza el nº de componentes y la varianza explicada por cada uno de ellos\n",
    "print('Nº de componentes:', len(pca_min_var_fitted.components_))\n",
    "print('Ratio de varianza explicada:')\n",
    "print(pca_min_var_fitted.explained_variance_ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Varía el valor de la varianza mínima a preservar y fíjate en el nº de componentes y la varianza explicada por cada uno de ellos.\n",
    "\n",
    "*¿Cuántos componentes se preservan con una varianza mínima del 100%? ¿Y del 95%, 90%, 85%, 75% y 50%?*\n",
    "\n",
    "*¿Cómo varían los ratios de varianza explicada en los primeros componentes devueltos en cada caso?*"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "common-cpu.m54",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m54"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
