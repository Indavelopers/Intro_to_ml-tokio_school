{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regresión logística: Función de coste y entrenamiento\n",
    "\n",
    "## ¿Qué vamos a hacer?\n",
    "- Crear un dataset sintético para regresión logística de forma manual y con Scikit-learn.\n",
    "- Implementar la función de actdivación logística sigmoide.\n",
    "- Implementar la función de coste regularizada para regresión logística.\n",
    "- Implementar el entrenamiento del modelo por gradient descent.\n",
    "- Comprobar el entrenamiento representando la evolución de la función de coste."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creación de un dataset sintético para regresión logística\n",
    "\n",
    "Vamos a crear un dataset sintético de nuevo, pero en esta ocasión para regresión logística.\n",
    "\n",
    "Vamos a descubrir cómo hacerlo con los 2 métodos que hemos usado previamente: de forma manual y con Scikit-learn, usando la función [sklearn_datasets.make_classification](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_classification.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Genera un dataset sintético con término de bias y error de forma manual\n",
    "m = 100\n",
    "n = 2\n",
    "\n",
    "# Genera un array 2D m x n con valores aleatorios entre -1 y 1\n",
    "# Insértale el término de bias como una primera columna de 1s\n",
    "X = [...]\n",
    "\n",
    "# Genera un array de theta de n + 1 valores aleatorios\n",
    "Theta_verd = [...]\n",
    "\n",
    "# Calcula Y en función de X y Theta_verd\n",
    "# Añádele un término de error modificable\n",
    "# Transforma Y a valores de 1. y 0. (float) cuando Y >= 0.5\n",
    "error = 0.15\n",
    "\n",
    "Y = [...]\n",
    "Y = [...]\n",
    "Y = [...]\n",
    "\n",
    "# Comprueba los valores y dimensiones de los vectores\n",
    "print('Theta a estimar:')\n",
    "print()\n",
    "\n",
    "print('Primeras 10 filas y 5 columnas de X e Y:')\n",
    "print()\n",
    "print()\n",
    "\n",
    "print('Dimensiones de X e Y:')\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Genera un dataset sintético con término de bias y error con Scikit-learn\n",
    "\n",
    "# Usa los mismos valores de m, n y error del dataset anterior\n",
    "X_sklearn = [...]\n",
    "Y_sklearn = [...]\n",
    "\n",
    "# Comprueba los valores y dimensiones de los vectores\n",
    "print('Primeras 10 filas y 5 columnas de X e Y:')\n",
    "print()\n",
    "print()\n",
    "\n",
    "print('Dimensiones de X e Y:')\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ya que con el método de Scikit-learn no podemos recuperar los coeficientes utilizados, vamos a usar el método manual."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementar la función sigmoide\n",
    "\n",
    "Vamos a implementar la función de activación sigmoide. Usaremos esta función para implementar nuestra hipótesis, que transforma las predicciones del modelo a valores de 0 y 1.\n",
    "\n",
    "Función sigmoide:\n",
    "\n",
    "$g(z) = \\frac{1}{1 + e^{-z}} \\\\\n",
    "Y = h_\\theta(x) = g(\\Theta \\times X) = \\frac{1}{1 + e^{-\\Theta^Tx}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implementa la función de activación sigmoide\n",
    "\n",
    "def sigmoid(theta, x):\n",
    "    \"\"\" Devuelve el valor del sigmoide para dicha theta y x\n",
    "    \n",
    "    Argumentos posicionales:\n",
    "    theta -- array 1D de Numpy con la fila o columna de coeficientes de las características\n",
    "    x -- array 1D de Numpy con las características de un ejemplo\n",
    "    \n",
    "    Devuelve:\n",
    "    sigmoide -- float con el valor del sigmoide para dichos parámetros\n",
    "    \"\"\"\n",
    "    \n",
    "    return [...]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementar la función de coste regularizada\n",
    "\n",
    "Vamos a implementar la función de coste regularizada. Esta función será similar a la que implementamos para regresión lineal en un ejercicio anterior.\n",
    "\n",
    "Función de coste regularizada:\n",
    "\n",
    "$J(\\Theta) = - [\\frac{1}{m} \\sum\\limits_{i=0}^{m} (y^i log(h_\\theta(x^i)) + (1 - y^i) log(1 - h_\\theta(x^i))] \\\\\n",
    "+ \\frac{\\lambda}{2m} \\sum_{j=1}^{n} \\Theta_j^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implementa la función de coste regularizada para regresión logística\n",
    "\n",
    "def regularized_logistic_cost_function(x, y, theta, lambda_=0.):\n",
    "    \"\"\" Computa la función de coste para el dataset y coeficientes considerados\n",
    "    \n",
    "    Argumentos posicionales:\n",
    "    x -- array 2D de Numpy con los valores de las variables independientes de los ejemplos, de tamaño m x n\n",
    "    y -- array 1D de Numpy con la variable dependiente/objetivo, de tamaño m x 1 y valores 0 o 1\n",
    "    theta -- array 1D de Numpy con los pesos de los coeficientes del modelo, de tamaño 1 x n (vector fila)\n",
    "    lambda_ -- factor de regularización, por defecto 0.\n",
    "    \n",
    "    Devuelve:\n",
    "    j -- float con el coste para dicho array theta\n",
    "    \"\"\"\n",
    "    m = [...]\n",
    "    \n",
    "    # Recuerda comprobar las dimensiones de la multiplicación matricial para hacerla correctamente\n",
    "    j = [...]\n",
    "    \n",
    "    # Regulariza para todo Theta excepto el término de bias (el primer valor)\n",
    "    j += [...]\n",
    "    \n",
    "    return j"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Del mismo modo que en ejercicios anteriores, comprueba tu implementación calculando la función de coste para cada ejemplo del dataset.\n",
    "\n",
    "Con la *Y* correcta y la *lambda* a 0, la función de coste debe ser también 0. Según se aleja la *theta* o se incrementa la *lambda*, el coste debe ser superior:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Comprueba tu implementación sobre el dataset\n",
    "\n",
    "theta = Theta_verd    # Modifica y comprueba varios valores de theta\n",
    "\n",
    "j = regularized_logistic_cost_function(X, Y, theta, lambda_=0.)\n",
    "\n",
    "print('Coste del modelo:')\n",
    "print(j)\n",
    "print('Theta comprobado y Theta real:')\n",
    "print(theta)\n",
    "print(Theta_verd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementar el entrenamiento por gradient descent\n",
    "\n",
    "Ahora vamos a optimizar dicha función de coste, a entrenar nuestro modelo por gradient descent regularizado.\n",
    "\n",
    "En el siguiente ejercicio usaremos la regularización para realizar validación cruzada.\n",
    "\n",
    "Actualizaciones de los coeficientes *theta*:\n",
    "\n",
    "$\\theta_0 := \\theta_0 - \\alpha \\frac{1}{m} \\sum\\limits_{i=0}^{m} (h_\\theta (x^i) - y^i) x_0^i \\\\\n",
    "\\theta_j := \\theta_j - \\alpha [\\frac{1}{m} \\sum\\limits_{i=0}^{m} (h_\\theta (x^i) - y^i) x_0^i + \\frac{\\lambda}{m} \\theta_j]; \\\\\n",
    "j \\in [1, n]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implementar la función que entrena el modelo por gradient descent regularizado\n",
    "\n",
    "def regularized_logistic_gradient_descent(x, y, theta, alpha=1e-1, lambda_=0., e=1e-3, iter_=1e3):\n",
    "    \"\"\" Entrena el modelo optimizando su función de coste por gradient descent\n",
    "    \n",
    "    Argumentos posicionales:\n",
    "    x -- array 2D de Numpy con los valores de las variables independientes de los ejemplos, de tamaño m x n\n",
    "    y -- array 1D de Numpy con la variable dependiente/objetivo, de tamaño m x 1\n",
    "    theta -- array 1D de Numpy con los pesos de los coeficientes del modelo, de tamaño 1 x n (vector fila)\n",
    "    \n",
    "    Argumentos nombrados (keyword):\n",
    "    alpha -- float, ratio de entrenamiento\n",
    "    lambda -- float con el parámetro de regularización\n",
    "    e -- float, diferencia mínima entre iteraciones para declarar que el entrenamiento ha convergido finalmente\n",
    "    iter_ -- int/float, nº de iteraciones\n",
    "    \n",
    "    Devuelve:\n",
    "    j_hist -- list/array con la evolución de la función de coste durante el entrenamiento\n",
    "    theta -- array de Numpy con el valor de theta en la última iteración\n",
    "    \"\"\"\n",
    "    iter_ = int(iter_)    # Si has declarado iter_ en notación científica (1e3) o float (1000.), conviértelo\n",
    "    \n",
    "    # Inicializa j_hist como una list o un array de Numpy. Recuerda que no sabemos qué tamaño tendrá finalmente\n",
    "    j_hist = [...]\n",
    "    \n",
    "    m, n = [...]    # Obtén m y n a partir de las dimensiones de X\n",
    "    \n",
    "    for k in [...]:    # Itera sobre el nº de iteraciones máximo\n",
    "        theta_iter = [...]    # Declara una theta para cada iteración, ya que debemos actualizarla\n",
    "        \n",
    "        for j in [...]:    # Itera sobre el nº de características\n",
    "            # Actualiza theta_iter para cada característica, según la derivada de la función de coste\n",
    "            # Incluye el ratio de entrenamiento alpha\n",
    "            # Cuidado con las multiplicaciones matriciales, su órden y dimensiones\n",
    "            \n",
    "            if j > 0:\n",
    "                pass    # Regulariza todo coeficiente excepto el del parámetro bias (primer valor)\n",
    "            \n",
    "            theta_iter[j] = theta[j] - [...]\n",
    "            \n",
    "        theta = theta_iter\n",
    "        \n",
    "        cost = regularized_logistic_cost_function([...])    # Calcula el coste para la iteración de theta actual\n",
    "        \n",
    "        j_hist[...]    # Añade el coste de la iteración actual al histórico de costes\n",
    "        \n",
    "        # Comprueba si la diferencia entre el coste de la iteración actual y el de la última iteración en valor\n",
    "        # absoluto son menores que la diferencia mínima para declarar convergencia, e\n",
    "        if k > 0 and [...]:\n",
    "            print('Converge en la iteración nº: ', k)\n",
    "            \n",
    "            break\n",
    "    else:\n",
    "        print('Nº máx. de iteraciones alcanzado')\n",
    "        \n",
    "    return j_hist, theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrenar un modelo de regresión logística no regularizado\n",
    "\n",
    "Para comprobar la implementación de tu función, úsala para entrenar un modelo de regresión logística sobre el dataset sintético sin regularizacón (*lambda* = 0).\n",
    "\n",
    "Comprueba que el modelo converge correctamente a *Theta_verd*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Comprueba tu implementación entrenando un modelo sobre el dataset sintético creado previamente\n",
    "\n",
    "# Crea una theta inicial con un valor dado.\n",
    "theta_ini = [...]\n",
    "\n",
    "print('Theta inicial:')\n",
    "print(theta_ini)\n",
    "\n",
    "alpha = 1e-1\n",
    "lambda_ = 0.\n",
    "e = 1e-3\n",
    "iter_ = 1e3\n",
    "\n",
    "print('Hiper-arámetros usados:')\n",
    "print('Alpha:', alpha, 'Error máx.:', e, 'Nº iter', iter_)\n",
    "\n",
    "t = time.time()\n",
    "j_hist, theta_final = regularized_logistic_gradient_descent([...])\n",
    "\n",
    "print('Tiempo de entrenamiento (s):', time.time() - t)\n",
    "\n",
    "# TODO: completar\n",
    "print('\\nÚltimos 10 valores de la función de coste')\n",
    "print(j_hist[...])\n",
    "print('\\Coste final:')\n",
    "print(j_hist[...])\n",
    "print('\\nTheta final:')\n",
    "print(theta_final)\n",
    "\n",
    "print('Valores verdaderos de Theta y diferencia con valores entrenados:')\n",
    "print(Theta_verd)\n",
    "print(theta_final - Theta_verd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Representar la evolución de la función de coste\n",
    "\n",
    "Para comprobar la evolución del entrenamiento de tu modelo, representa gráficamente el histórico de la función de coste:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TOOD: Representa gráficamente la función de coste vs el nº de iteraciones\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.title('Función de coste')\n",
    "plt.xlabel('nº iteraciones')\n",
    "plt.ylabel('coste')\n",
    "\n",
    "plt.plot([...])    # Completa\n",
    "\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "common-cpu.m54",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m54"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
